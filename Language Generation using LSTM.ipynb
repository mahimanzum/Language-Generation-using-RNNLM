{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"pr691RPFo4_0","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"G9peD27d6r4-","colab_type":"code","outputId":"ec7f8b55-84ed-4e1f-e1ee-7d37bf03dfc8","executionInfo":{"status":"ok","timestamp":1549076484281,"user_tz":-360,"elapsed":62167,"user":{"displayName":"Mahim Anzum Haque pantho","photoUrl":"https://lh4.googleusercontent.com/-t-OANPV3UrY/AAAAAAAAAAI/AAAAAAAAJW8/eEwnrZoH7dM/s64/photo.jpg","userId":"07659887497514272211"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"cell_type":"code","source":["#Block 1 server setup \n","!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"oGXQPOZw64Do","colab_type":"code","colab":{}},"cell_type":"code","source":["#Block 2 sync colabserver with drive\n","!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zQmI-syx9aMs","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install -q tensorflow"],"execution_count":0,"outputs":[]},{"metadata":{"id":"712Jduaa9jrp","colab_type":"code","outputId":"9186924f-f517-4274-db34-1844b822e231","executionInfo":{"status":"ok","timestamp":1549076496278,"user_tz":-360,"elapsed":5193,"user":{"displayName":"Mahim Anzum Haque pantho","photoUrl":"https://lh4.googleusercontent.com/-t-OANPV3UrY/AAAAAAAAAAI/AAAAAAAAJW8/eEwnrZoH7dM/s64/photo.jpg","userId":"07659887497514272211"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["#ls \"drive/ML Project/\"\n","\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"metadata":{"id":"1Z3X-W9GA-YO","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"LTO84OaF-XnA","colab_type":"code","outputId":"6b78a29f-aabd-4890-f6a1-77b9aea9bbe1","executionInfo":{"status":"ok","timestamp":1549076496281,"user_tz":-360,"elapsed":1648,"user":{"displayName":"Mahim Anzum Haque pantho","photoUrl":"https://lh4.googleusercontent.com/-t-OANPV3UrY/AAAAAAAAAAI/AAAAAAAAJW8/eEwnrZoH7dM/s64/photo.jpg","userId":"07659887497514272211"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["#Block 3 import libraries\n","from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers.advanced_activations import ELU\n","from keras.preprocessing.text import Tokenizer\n","from keras.callbacks import ModelCheckpoint\n","from keras.optimizers import Adam\n","from keras import backend as K\n","from keras.models import Model\n","from scipy import spatial\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import codecs\n","import csv\n","import os\n","import random\n","SEED = 30\n","random.seed(SEED)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"5vz4eHJg_dt-","colab_type":"code","outputId":"bd1b5c88-e0b5-483b-f670-b5d8baece1eb","executionInfo":{"status":"ok","timestamp":1549076552684,"user_tz":-360,"elapsed":54526,"user":{"displayName":"Mahim Anzum Haque pantho","photoUrl":"https://lh4.googleusercontent.com/-t-OANPV3UrY/AAAAAAAAAAI/AAAAAAAAJW8/eEwnrZoH7dM/s64/photo.jpg","userId":"07659887497514272211"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"cell_type":"code","source":["#block 4 data preprocessing\n","BASE_DIR = 'drive/ML Project/'\n","TRAIN_DATA_FILE = BASE_DIR + 'train.csv'#'train_micro.csv'\n","GLOVE_EMBEDDING = BASE_DIR + 'glove.6B.300d.txt'\n","VALIDATION_SPLIT = 0.2\n","MAX_SEQUENCE_LENGTH = 15\n","MAX_NB_WORDS = 20000\n","EMBEDDING_DIM = 300\n","\n","texts = []\n","with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n","    reader = csv.reader(f, delimiter=',')\n","    header = next(reader)\n","    for values in reader:\n","        texts.append(values[3])\n","        texts.append(values[4])\n","print('Found %s texts in train.csv' % len(texts))\n","\n","tokenizer = Tokenizer(MAX_NB_WORDS)\n","tokenizer.fit_on_texts(texts)\n","word_index = tokenizer.word_index #the dict values start from 1 so this is fine with zeropadding\n","index2word = {v: k for k, v in word_index.items()}\n","print('Found %s unique tokens' % len(word_index))\n","sequences = tokenizer.texts_to_sequences(texts)\n","data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post',truncating='post')\n","print('Shape of data tensor:', data_1.shape)\n","NB_WORDS = (min(tokenizer.num_words, len(word_index)) + 1 ) #+1 for zero padding\n","#data_1_val = data_1[801000:807000] #select 6000 sentences as validation data\n","\n","data_x = np.copy(data_1)\n","for i in range(len(data_x)):\n","  rd = random.randint(0,(MAX_SEQUENCE_LENGTH-1)//2)\n","  data_x[i][rd] = word_index['pad']\n","\n","data_val_y = data_1[775000:783000]\n","data_train_y = data_1[:775000]\n","\n","data_val_x = data_x[775000:783000]\n","data_train_x = data_x[:775000]\n","\n","def give():\n","  data_x = np.copy(data_1)\n","  for i in range(len(data_x)):\n","    rd = random.randint(1,(MAX_SEQUENCE_LENGTH-1)//2)\n","    #data_x[i][rd] = word_index['pad']\n","  data_val_y = data_1[775000:783000]\n","  data_train_y = data_1[:775000]\n","  data_val_x = data_x[775000:783000]\n","  data_train_x = data_x[:775000]\n","  return data_train_x,data_train_y, data_val_x, data_val_y\n","data_train_x,data_train_y, data_val_x, data_val_y = give()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 808580 texts in train.csv\n","Found 95596 unique tokens\n","Shape of data tensor: (808580, 15)\n"],"name":"stdout"}]},{"metadata":{"id":"fVmh5-eUEo5J","colab_type":"code","outputId":"7ceb7665-cde7-494b-dbc4-6bddc114423b","executionInfo":{"status":"ok","timestamp":1549076552686,"user_tz":-360,"elapsed":51312,"user":{"displayName":"Mahim Anzum Haque pantho","photoUrl":"https://lh4.googleusercontent.com/-t-OANPV3UrY/AAAAAAAAAAI/AAAAAAAAJW8/eEwnrZoH7dM/s64/photo.jpg","userId":"07659887497514272211"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"cell_type":"code","source":["print(word_index['pad'])\n","print(index2word[10291])\n","print(random.randint(0,MAX_SEQUENCE_LENGTH-1))\n","print(data_x[2])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10291\n","pad\n","4\n","[    2     3     1 10291    10 14300 13598     5  4565     0     0     0\n","     0     0     0]\n"],"name":"stdout"}]},{"metadata":{"id":"yZyD_UT4AppD","colab_type":"code","outputId":"83c489a5-8c6a-4ffe-b3b4-e801c1c581c5","executionInfo":{"status":"ok","timestamp":1549076596250,"user_tz":-360,"elapsed":91246,"user":{"displayName":"Mahim Anzum Haque pantho","photoUrl":"https://lh4.googleusercontent.com/-t-OANPV3UrY/AAAAAAAAAAI/AAAAAAAAJW8/eEwnrZoH7dM/s64/photo.jpg","userId":"07659887497514272211"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":["#block 5 Loading GloVe matrix\n","'''\n","mx = 0\n","idx = 0\n","for i in range(len(sequences)):\n","    if mx <len(sequences[i]):\n","        mx = len(sequences[i]);\n","        idx = i\n","print(idx)\n","print(texts[36111])\n","'''\n","def sent_generator(TRAIN_DATA_FILE, chunksize):\n","    reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=chunksize, iterator=True)\n","    for df in reader:\n","        # print(df.shape)\n","        # df=pd.read_csv(TRAIN_DATA_FILE, iterator=False)\n","        val3 = df.iloc[:, 3:4].values.tolist()\n","        val4 = df.iloc[:, 4:5].values.tolist()\n","        flat3 = [item for sublist in val3 for item in sublist]\n","        flat4 = [str(item) for sublist in val4 for item in sublist]\n","        texts = []\n","        texts.extend(flat3[:])\n","        texts.extend(flat4[:])\n","\n","        sequences = tokenizer.texts_to_sequences(texts)\n","        data_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post',truncating='post')\n","        yield [data_train, data_train]\n","\n","embeddings_index = {}\n","f = open(GLOVE_EMBEDDING, encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    if i < NB_WORDS:\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            # words not found in embedding index will be the word embedding of 'unk'.\n","            glove_embedding_matrix[i] = embedding_vector\n","        else:\n","            glove_embedding_matrix[i] = embeddings_index.get('unk')\n","print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 400000 word vectors.\n","Null word embeddings: 1\n"],"name":"stdout"}]},{"metadata":{"id":"zH3nwFXGCZLc","colab_type":"code","outputId":"bcacb692-8112-41b9-a387-3641e0861b6a","executionInfo":{"status":"ok","timestamp":1549076620691,"user_tz":-360,"elapsed":111452,"user":{"displayName":"Mahim Anzum Haque pantho","photoUrl":"https://lh4.googleusercontent.com/-t-OANPV3UrY/AAAAAAAAAAI/AAAAAAAAJW8/eEwnrZoH7dM/s64/photo.jpg","userId":"07659887497514272211"}},"colab":{"base_uri":"https://localhost:8080/","height":557}},"cell_type":"code","source":["#block 6 contruction of model architecture\n","batch_size = 500\n","max_len = MAX_SEQUENCE_LENGTH\n","emb_dim = EMBEDDING_DIM\n","latent_dim = 512\n","intermediate_dim = 1024\n","epsilon_std = 1.0\n","num_sampled= 500\n","kl_weight = 0.001\n","\n","act = ELU()\n","\n","#y = Input(batch_shape=(None, max_len, NB_WORDS))\n","x = Input(batch_shape=(None, max_len))\n","x_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix],\n","                            input_length=max_len, trainable=False)(x)\n","h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)\n","#https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n","\n","#h = Bidirectional(LSTM(intermediate_dim, return_sequences=False), merge_mode='concat')(h)\n","#h = Dropout(0.2)(h)\n","#h = Dense(intermediate_dim, activation='linear')(h)\n","#h = act(h)\n","#h = Dropout(0.2)(h)\n","z_mean = Dense(latent_dim)(h)\n","z_log_var = Dense(latent_dim)(h)\n","\n","def sampling(args):\n","    z_mean, z_log_var = args\n","    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n","                              stddev=epsilon_std, seed=SEED)\n","    return z_mean + K.exp(z_log_var / 2) * epsilon\n","\n","# note that \"output_shape\" isn't necessary with the TensorFlow backend\n","z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n","\n","# we instantiate these layers separately so as to reuse them later\n","repeated_context = RepeatVector(max_len)\n","decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n","decoder_mean = TimeDistributed(Dense(NB_WORDS, activation='linear'))#https://keras.io/layers/wrappers/  #softmax is applied in the seq2seqloss by tf\n","\n","#https://github.com/keras-team/keras/issues/5203\n","h_decoded = decoder_h(repeated_context(z))\n","x_decoded_mean = decoder_mean(h_decoded)\n","\n","#https://stackoverflow.com/questions/46663013/what-is-y-true-and-y-pred-when-creating-a-custom-metric-in-keras\n","# placeholder loss\n","def zero_loss(y_true, y_pred):\n","    return K.zeros_like(y_pred)\n","\n","  \n","#https://stackoverflow.com/questions/50063613/add-loss-function-in-keras\n","# Custom loss layer\n","class CustomVariationalLayer(Layer):\n","    def __init__(self, **kwargs):\n","        self.is_placeholder = True\n","        super(CustomVariationalLayer, self).__init__(**kwargs)\n","        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n","\n","    def vae_loss(self, x, x_decoded_mean):\n","        #xent_loss = K.sum(metrics.categorical_crossentropy(x, x_decoded_mean), axis=-1)\n","        labels = tf.cast(x, tf.int32)\n","        #https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/\n","        xent_loss = K.sum(tf.contrib.seq2seq.sequence_loss(x_decoded_mean, labels, \n","                                                     weights=self.target_weights,\n","                                                     average_across_timesteps=False,\n","                                                     average_across_batch=False), axis=-1)#,\n","                                                     #softmax_loss_function=softmax_loss_f), axis=-1)#,\n","        #https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n","        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n","        xent_loss = K.mean(xent_loss)\n","        kl_loss = K.mean(kl_loss)\n","        return 100*xent_loss + kl_weight * kl_loss\n","    \n","    def call(self, inputs):\n","        x = inputs[0]\n","        x_decoded_mean = inputs[1]\n","        print(x.shape, x_decoded_mean.shape)\n","        loss = self.vae_loss(x, x_decoded_mean)\n","        self.add_loss(loss, inputs=inputs)\n","        # we don't use this output, but it has to have the correct shape:\n","        return K.ones_like(x)\n","    \n","def kl_loss(x, x_decoded_mean):\n","    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n","    kl_loss = kl_weight * kl_loss\n","    return kl_loss\n","\n","#https://github.com/keras-team/keras/issues/5563 # about custom layer\n","#https://keras.io/layers/writing-your-own-keras-layers/ very important\n","loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n","#https://github.com/keras-team/keras/issues/7395\n","vae = Model(x, [loss_layer])\n","opt = Adam(lr=0.01) \n","vae.compile(optimizer='adam', loss=[zero_loss], metrics=[kl_loss])\n","vae.summary()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(?, 15) (?, 15, 20001)\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 15)           0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 15, 300)      6000300     input_1[0][0]                    \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 2048)         10854400    embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 512)          1049088     bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 512)          1049088     bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 512)          0           dense_1[0][0]                    \n","                                                                 dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","repeat_vector_1 (RepeatVector)  (None, 15, 512)      0           lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   (None, 15, 1024)     6295552     repeat_vector_1[0][0]            \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 15, 20001)    20501025    lstm_2[0][0]                     \n","__________________________________________________________________________________________________\n","custom_variational_layer_1 (Cus [(None, 15), (None,  0           input_1[0][0]                    \n","                                                                 time_distributed_1[0][0]         \n","==================================================================================================\n","Total params: 45,749,453\n","Trainable params: 39,749,153\n","Non-trainable params: 6,000,300\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"Ge3YjD0KGTjE","colab_type":"code","outputId":"734abd17-f6b7-47a2-f30e-f4d14545cbee","executionInfo":{"status":"error","timestamp":1548904110489,"user_tz":-360,"elapsed":3052652,"user":{"displayName":"Mahim Anzum Haque pantho","photoUrl":"https://lh4.googleusercontent.com/-t-OANPV3UrY/AAAAAAAAAAI/AAAAAAAAJW8/eEwnrZoH7dM/s64/photo.jpg","userId":"07659887497514272211"}},"colab":{"base_uri":"https://localhost:8080/","height":1237}},"cell_type":"code","source":["#block 7 model training and saving\n","def create_model_checkpoint(dir, model_name):\n","    filepath = dir + '/' + model_name + \".h5\"  # -{epoch:02d}-{decoded_mean:.2f}\n","    directory = os.path.dirname(filepath)\n","    try:\n","        os.stat(directory)\n","    except:\n","        os.mkdir(directory)\n","    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n","    return checkpointer\n","\n","checkpointer = create_model_checkpoint(BASE_DIR+'models', 'vae_lstm_final_2')\n","\n","np_epochs = 10\n","for i in range(np_epochs):\n","    #vae.load_weights(BASE_DIR+'models/vae_lstm_final_2.h5')\n","    data_train_x,data_train_y, data_val_x, data_val_y = give()\n","    vae.fit(data_train_x, data_train_y,\n","       shuffle=True,\n","       epochs=1,\n","       batch_size=batch_size,\n","       validation_data=(data_val_x, data_val_y), callbacks=[checkpointer])\n","    vae.save(BASE_DIR+'models/vae_lstm_final_2.h5')\n","\n","    \n","# build a model to project sentences on the latent space\n","encoder = Model(x, z_mean)\n","# build a generator that can sample sentences from the learned distribution\n","decoder_input = Input(shape=(latent_dim,))\n","_h_decoded = decoder_h(repeated_context(decoder_input))\n","_x_decoded_mean = decoder_mean(_h_decoded)\n","_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n","generator = Model(decoder_input, _x_decoded_mean)\n","encoder.save(BASE_DIR+'models/vae_encoder_final_2.h5')\n","generator.save(BASE_DIR+'models/vae_generator_final_2.h5')\n","\n","\n","'''\n","nb_epoch = 2\n","n_steps = (800000 / 2) / batch_size  # we use the first 800000\n","\n","for counter in range(nb_epoch):\n","    print('-------epoch: ', counter, '--------')\n","    vae.fit_generator(sent_generator(TRAIN_DATA_FILE, batch_size / 2),\n","                      steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer],\n","                      validation_data=(data_1_val, data_1_val))\n","\n","vae.fit_generator(sent_generator(TRAIN_DATA_FILE, batch_size / 2),\n","                      steps_per_epoch=n_steps, epochs=nb_epoch, callbacks=[checkpointer],\n","                      validation_data=(data_1_val, data_1_val))\n","'''\n","#vae.save(BASE_DIR+'models/vae_lstm800k32dim96hid.h5')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 775000 samples, validate on 8000 samples\n","Epoch 1/1\n","775000/775000 [==============================] - 1639s 2ms/step - loss: 81.5664 - kl_loss: 1.8406 - val_loss: 191.1367 - val_kl_loss: 1.8303\n","\n","Epoch 00001: val_loss improved from inf to 191.13672, saving model to drive/ML Project/models/vae_lstm_final.h5\n","Train on 775000 samples, validate on 8000 samples\n","Epoch 1/1\n","775000/775000 [==============================] - 1633s 2ms/step - loss: 68.1229 - kl_loss: 1.7758 - val_loss: 194.0846 - val_kl_loss: 1.7783\n","\n","Epoch 00001: val_loss did not improve from 191.13672\n","Train on 775000 samples, validate on 8000 samples\n","Epoch 1/1\n","775000/775000 [==============================] - 1635s 2ms/step - loss: 70.6391 - kl_loss: 1.7964 - val_loss: 194.5061 - val_kl_loss: 1.7597\n","\n","Epoch 00001: val_loss did not improve from 191.13672\n","Train on 775000 samples, validate on 8000 samples\n","Epoch 1/1\n","775000/775000 [==============================] - 1634s 2ms/step - loss: 69.6839 - kl_loss: 1.7776 - val_loss: 194.3264 - val_kl_loss: 1.7457\n","\n","Epoch 00001: val_loss did not improve from 191.13672\n","Train on 775000 samples, validate on 8000 samples\n","Epoch 1/1\n","147500/775000 [====>.........................] - ETA: 21:57 - loss: 66.3900 - kl_loss: 1.7797"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-cda261ca0bda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m        validation_data=(data_val_x, data_val_y), callbacks=[checkpointer])\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m#vae.save(BASE_DIR+'models/vae_lstm_final.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"FIFOFL43lQgf","colab_type":"code","outputId":"1341751f-64b9-432a-93da-60562110651b","executionInfo":{"status":"error","timestamp":1549076638512,"user_tz":-360,"elapsed":121219,"user":{"displayName":"Mahim Anzum Haque pantho","photoUrl":"https://lh4.googleusercontent.com/-t-OANPV3UrY/AAAAAAAAAAI/AAAAAAAAJW8/eEwnrZoH7dM/s64/photo.jpg","userId":"07659887497514272211"}},"colab":{"base_uri":"https://localhost:8080/","height":910}},"cell_type":"code","source":["#block 8 loading pretrained model weights\n","vae.load_weights(BASE_DIR+'models/vae_lstm_final.h5')\n","encoder = Model(x, z_mean)\n","# build a generator that can sample sentences from the learned distribution\n","decoder_input = Input(shape=(latent_dim,))\n","_h_decoded = decoder_h(repeated_context(decoder_input))\n","_x_decoded_mean = decoder_mean(_h_decoded)\n","_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n","generator = Model(decoder_input, _x_decoded_mean)\n","encoder.save(BASE_DIR+'models/vae_encoder_final.h5')\n","generator.save(BASE_DIR+'models/vae_generator_final.h5')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-ac64dc1e0fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x_decoded_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'models/vae_encoder_final.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'models/vae_generator_final.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0m_serialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_serialize_model\u001b[0;34m(model, f, include_optimizer)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mlayer_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mlayer_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_optimizer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, attr, val)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# Check that no item in `data` is larger than `HDF5_OBJECT_HEADER_LIMIT`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNLIMITED\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"ufyfbhl4JPV0","colab_type":"code","outputId":"39a4a8f6-e5fb-4946-b91f-0610081be674","executionInfo":{"status":"ok","timestamp":1549076660710,"user_tz":-360,"elapsed":10302,"user":{"displayName":"Mahim Anzum Haque pantho","photoUrl":"https://lh4.googleusercontent.com/-t-OANPV3UrY/AAAAAAAAAAI/AAAAAAAAJW8/eEwnrZoH7dM/s64/photo.jpg","userId":"07659887497514272211"}},"colab":{"base_uri":"https://localhost:8080/","height":161}},"cell_type":"code","source":["#block 9 testing functions\n","encoder.load_weights(BASE_DIR+'models/vae_encoder.h5')\n","generator.load_weights(BASE_DIR+'models/vae_generator.h5')\n","\n","def print_sentence(sent_vect):\n","    word_list = list(np.vectorize(index2word.get)(sent_vect))\n","    w_list = [w for w in word_list] # if w not in ['pad']]\n","    print(' '.join(w_list))\n","    #print(word_list)\n","index2word = {v: k for k, v in word_index.items()}\n","index2word[0] = 'pad'\n","'''\n","#test on a validation sentence\n","sent_idx = 100\n","print_sentence(data_val[sent_idx])\n","#print(data_val[sent_idx])\n","sent_encoded = encoder.predict(data_val[sent_idx:sent_idx+2,:])\n","x_test_reconstructed = generator.predict(sent_encoded, batch_size = 1)\n","reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[0])\n","np.apply_along_axis(np.max, 1, x_test_reconstructed[0])\n","np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[0]))\n","word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n","print(' '.join(word_list))\n","original_sent = list(np.vectorize(index2word.get)(data_val[sent_idx]))\n","print(' '.join(original_sent))\n","'''\n","\n","# function to parse a sentence\n","def sent_parse(sentence, mat_shape):\n","    sequence = tokenizer.texts_to_sequences(sentence)\n","    padded_sent = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post',truncating='post')\n","    return padded_sent#[padded_sent, sent_one_hot]\n","\n","\n","# input: encoded sentence vector\n","# output: encoded sentence vector in dataset with highest cosine similarity\n","'''\n","def find_similar_encoding(sent_vect):\n","    all_cosine = []\n","    for sent in sent_encoded:\n","        result = 1 - spatial.distance.cosine(sent_vect, sent)\n","        all_cosine.append(result)\n","    data_array = np.array(all_cosine)\n","    maximum = data_array.argsort()[-3:][::-1][1]\n","    new_vec = sent_encoded[maximum]\n","    return new_vec\n","'''\n","def find_similar_encoding(sent_vect):\n","    dis = 100\n","    global ret\n","    for sent in encoder.predict(data_train):\n","        result = 1 - spatial.distance.cosine(sent_vect, sent)\n","        if(result<dis):\n","          dis = result\n","          ret = sent\n","    return ret\n","\n","# input: two points, integer n\n","# output: n equidistant points on the line between the input points (inclusive)\n","def shortest_homology(point_one, point_two, num):\n","    dist_vec = point_two - point_one\n","    sample = np.linspace(0, 1, num, endpoint = True)\n","    hom_sample = []\n","    for s in sample:\n","        hom_sample.append(point_one + s * dist_vec)\n","    return hom_sample\n","\n","\n","\n","# input: original dimension sentence vector\n","# output: sentence text\n","def print_latent_sentence(sent_vect):\n","    sent_vect = np.reshape(sent_vect,[1,latent_dim])\n","    sent_reconstructed = generator.predict(sent_vect)\n","    sent_reconstructed = np.reshape(sent_reconstructed,[max_len,NB_WORDS])\n","    reconstructed_indexes = np.apply_along_axis(np.argmax, 1, sent_reconstructed)\n","    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n","    w_list = [w for w in word_list if w not in ['pad']]\n","    print(' '.join(w_list))\n","    #print(word_list)\n","    \n","       \n","        \n","def new_sents_interp(sent1, sent2, n):\n","    tok_sent1 = sent_parse(sent1, [30])\n","    tok_sent2 = sent_parse(sent2, [30])\n","    enc_sent1 = encoder.predict(tok_sent1, batch_size = 16)\n","    enc_sent2 = encoder.predict(tok_sent2, batch_size = 16)\n","    test_hom = shortest_homology(enc_sent1, enc_sent2, n)\n","    for point in test_hom:\n","        print_latent_sentence(point)\n","\n","\n","print(\"intepolation starts\")\n","#====================== Example ====================================#\n","sentence1=['where can i find a book on machine learning']\n","mysent = sent_parse(sentence1, [27])\n","mysent_encoded = encoder.predict(mysent, batch_size = 16)\n","print_latent_sentence(mysent_encoded)\n","#print_latent_sentence(find_similar_encoding(mysent_encoded))\n","\n","sentence2=['how can i become a successful entrepreneur']\n","mysent2 = sent_parse(sentence2, [27])\n","mysent_encoded2 = encoder.predict(mysent2, batch_size = 16)\n","#print_latent_sentence(mysent_encoded2)\n","#print_latent_sentence(find_similar_encoding(mysent_encoded2))\n","#print('-----------------')\n","\n","new_sents_interp(sentence1, sentence2, 6)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["intepolation starts\n","where can i find a book on machine learning\n","where can i find a book on machine learning\n","where can i find a book on machine learning\n","where can i find a successful on machine\n","how can i become a successful developer hacker\n","how can i become a successful entrepreneur\n","how can i become a successful entrepreneur\n"],"name":"stdout"}]},{"metadata":{"id":"nwos2cshJQUp","colab_type":"code","outputId":"fdacb2e3-3009-4996-cdb3-b3a92d8c04d5","executionInfo":{"status":"ok","timestamp":1549076663153,"user_tz":-360,"elapsed":897,"user":{"displayName":"Mahim Anzum Haque pantho","photoUrl":"https://lh4.googleusercontent.com/-t-OANPV3UrY/AAAAAAAAAAI/AAAAAAAAJW8/eEwnrZoH7dM/s64/photo.jpg","userId":"07659887497514272211"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["#block 10 sample tests\n","sentence1 =['what is java pad how to pad java programming language']\n","sentence2 =['how can you pad a blu ray dvd on a regular dvd player']\n","sentence3 =['How does pad printing work?']\n","def fill_pad(sentence):\n","  mysent = sent_parse(sentence, [27])\n","  encoded = encoder.predict(mysent, batch_size = 16)\n","  print_latent_sentence(encoded)\n","fill_pad(sentence3)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["how does the printing work\n"],"name":"stdout"}]},{"metadata":{"id":"yNvL1WtGJRAu","colab_type":"code","outputId":"8a301dbe-d947-4472-c2d1-18a3b79155f9","executionInfo":{"status":"ok","timestamp":1548904422832,"user_tz":-360,"elapsed":1123,"user":{"displayName":"Mahim Anzum Haque pantho","photoUrl":"https://lh4.googleusercontent.com/-t-OANPV3UrY/AAAAAAAAAAI/AAAAAAAAJW8/eEwnrZoH7dM/s64/photo.jpg","userId":"07659887497514272211"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["print_sentence(data_train_y[103])\n","print_sentence(data_train_x[102])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["how can you play a blu ray dvd on a regular dvd player pad pad\n","will a blu ray play on a regular dvd player if so how pad pad\n"],"name":"stdout"}]},{"metadata":{"id":"k7Utijy0JRmM","colab_type":"code","colab":{}},"cell_type":"code","source":["|'how can i pad the speed of my internet connection while using a vpn'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uGv2DGS9JSL3","colab_type":"code","colab":{}},"cell_type":"code","source":["#'where can i pad some water'"],"execution_count":0,"outputs":[]}]}